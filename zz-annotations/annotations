


Commands
************************
Podman
************************
-- Login quay.io
podman login quay.io

-- Logout
podman logout quay.io --all

-- Build a image
podman build --layers=false -t do288-hello-java .


-- Tag a image
podman tag do288-hello-java quay.io/mwielganczuk/do288-hello-java

-- push a image to registry
podman push --format v2s1 quay.io/mwielganczuk/do288-hello-java

-- Remove all images
podman rmi -a --force

-- run a image
podman run -d --name hello-java quay.io/mwielganczuk/do288-hello-java 

-- stop the container
podman stop pod_id

-- remove the container
podman rm pod_id

-- show all containers
podman ps -a


-- extract a podman image as oci format. Used to export to open shift repository
podman save --format oci-dir -o ./hello-java localhost/do288-hello-java



************************
Skopeo
************************
- Inspect a image
skopeo inspect docker://quay.io/mwielganczuk/do288-hello-java



************************
Openshift
************************
- Delete project
oc delete project name_project

- Show the pods
oc get pods

- Show the logs of deployment
oc logs pod_id

- Show the routes
oc get routes

- Show the status of project
oc status

- Expose the service for the outside the cluster
- Create a route for the service
oc expose svc/elvis

- Create a map config
oc create cm appconfig --from-literal APP_MSG="Elvis lives"


- Describe resources
oc describe cm/appconfig

- Set the config maps to deployment
oc set env deployment/elvis --from cm/appconfig

- Create a new app using a docker image
oc new-app --name elvis quay.io/mwielganczuk/do288-hello-java

- Create a project
oc new-project design-container
oc new-app --name sleep --image quay.io/mwielganczuk/do288-hello-java
--name sleep -> name of application


-- delete the project
oc delete all -l app=myapp



-- login on local openshift
oc login -u developer -p developer url_api

-- Create a secret from the container registry API access token that was stored by Podman.
oc create secret generic quayio \
--from-file .dockerconfigjson=${XDG_RUNTIME_DIR}/containers/auth.json \
--type kubernetes.io/dockerconfigjson

-- Link the new secret to the default service account.
oc secrets link default quayio --for pull


oc new-app --name myapp --build-env npm_config_registry=http://registry.npmjs.org/repository/nodejs nodejs:16-ubi8~https://github.com/marciodevelopment/DO288-apps#app-config --context-dir app-config




https://access.redhat.com/RegistryAuthentication

- Authenticating OpenShift to Private Registries
- You can provide your private registry credentials directly to the oc create secret command:

oc create secret docker-registry registrycreds \
--docker-server registry.example.com \
--docker-username youruser \
--docker-password y

- Another way of creating the secret is to use the authentication token from the podman login command:
oc create secret generic registrycreds \
--from-file .dockerconfigjson=${XDG_RUNTIME_DIR}/containers/auth.json \
--type kubernetes.io/dockerconfigjson

- Link the secret to the default service account from your project:
oc secrets link default registrycreds --for pull

- To use the secret to access an S2I builder image, link the secret to the builder service account from your project:
oc secrets link builder registrycreds


-- Change the spec.defaultRoute attribute to true, and the Image Registry operator creates a route to expose the internal registry
oc patch config.imageregistry cluster -n openshift-image-registry \
--type merge -p '{"spec":{"defaultRoute":true}}'

- Get the route of registry
oc get route -n openshift-image-registry


-- Authenticating to an Internal Registry
- fetch the open shift token
TOKEN=$(oc whoami -t)

-Use the token as part of a login subcommand from Podman:
podman login -u myuser -p ${TOKEN} \
default-route-openshift-image-registry.domain.example.com

- example of laptop
podman login -u kubeadmin -p ${TOKEN} default-route-openshift-image-registry.apps-crc.testing

-- You can also use the token as the value of the --[src|dst]creds options from Skopeo.
skopeo inspect --creds=myuser:${TOKEN} \
docker://default-route-openshift-image-registry.domain.example.com/...

skopeo inspect --creds=kubeadmin:${TOKEN} docker://default-route-openshift-image-registry.apps-crc.testing


skopeo inspect docker://default-route-openshift-image-registry.apps-crc.testing
skopeo inspect --tls-verify=false docker://default-route-openshift-image-registry.apps-crc.testing


sudo mkdir -p /etc/containers/certs.d/default-route-openshift-image-registry.apps-crc.testing
sudo cp registry-ca.crt /etc/containers/certs.d/default-route-openshift-image-registry.apps-crc.testing/ca.crt

-- Granting Access to Images in an Internal Registry
oc policy add-role-to-user system:image-puller \
user_name -n project_name

-- These roles allow users to pull and inspect images from the internal registry.
registry-viewer and system:image-puller

-- These roles allow users to push and tag images to the internal registry.
registry-editor and system:image-pusher
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.10/html-single/registry/index#accessing-the-registry

INTERNAL_REGISTRY=default-route-openshift-image-registry.apps-crc.testing
https://medium.com/@mgreenbe_84803/using-openshifts-internal-registry-e4a81d09da59


-- show the images on Image Stream
oc get is

-- Copy the image to IS
skopeo copy --format v2s1 \
--dest-creds=kubeadmin:${TOKEN} \
--tls-verify=false \
oci:hello-java \
docker://${REGISTRY}/${RHT_OCP4_DEV_USER}-common/hello-java:1.0


-- Show the images from openshift
 oc get is -n openshift -o name
 oc get istag -n openshift | grep php 
oc describe is php -n openshift

-- Create the hello-world image stream that points to the redhattraining/hello-world-nginx container image from Quay.io:
oc import-image hello-world --confirm --from quay.io/redhattraining/hello-world-nginx

-- Verify if its created
oc get istag

--Describe the container image
oc describe is hello-world

-- Show the images on the openshift
oc get is -n openshift


---------------------- Managing Application Builds -------------------
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.10/html-single/cicd/index#basic-build-operations
-- Starts a new build manually. The build configuration resource name is the only required argument to start a new build.
-- A succesfull build creates a new container image in the output ImageStreamTag. If a deployment configuration defines a trigger on that ImageStreamTag, the deployment process starts.
oc start-build name

-- Cancels a build. only possible to cancel builds that are in running or pending state
oc cancel-build name

-- Deletes a build configuration
oc delete bc/name

-- Describes details about a build configuration resource and the associated builds
oc describe bc name

-- Shows the build logs. You can check if your application is building correctly.
-- The -f option follows the log until you terminate the command with Ctrl+C.
oc logs -f bc/name
-- Display the build logs from a specific build:
oc logs build/name-1

Pruning Builds
apiVersion: "v1"
kind: "BuildConfig"
metadata:
  name: "sample-build"
spec:
  successfulBuildsHistoryLimit: 2
  failedBuildsHistoryLimit: 2
  ...contents omitted...

-- Log Verbosity
oc set env bc/name BUILD_LOGLEVEL="4" ## 0 - 5


-- Create a application from source
oc new-app --name jhost \ 1 - Create a new app
--build-env MAVEN_MIRROR_URL=http://${RHT_OCP4_NEXUS_SERVER}/repository/java \ -- set the env variable
-i redhat-openjdk18-openshift:1.8 \  -- Specifies the image to be used for the build
https://github.com/${RHT_OCP4_GITHUB_USER}/DO288-apps#manage-builds \
--context-dir java-serverhost -- Specifies a subdirectory within the source repository (java-serverhost) as the build context

oc new-app --name usuario-app \
--build-env MAVEN_MIRROR_URL=https://repo1.maven.org/maven2 \
-i ubi8-openjdk-17:1.11 \
https://github.com/marciodevelopment/usuario-op#manage-builds


oc new-app --name usuario-app-spring \
--build-env MAVEN_MIRROR_URL=https://repo1.maven.org/maven2 \
-i java-springboot-image:latest \
https://github.com/marciodevelopment/usuario-op#manage-builds


oc new-app --name usuario-app-spring \
--build-env MAVEN_MIRROR_URL=https://repo1.maven.org/maven2 \
-i java-springboot-image:latest \
https://github.com/devfile-samples/devfile-sample-java-springboot-basic


-- show the builds configuration
oc get bc

--List all builds available in the project:
oc get builds

-- Start a new build with the oc start-build command:
oc start-build bc/jhost

--------- Triggering Builds -------------
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.10/html-single/cicd/index#triggering-builds-build-hooks
** To view the triggers associated with a build configuration
oc describe bc/name

** To add an image change trigger to a build configuration, use the oc set triggers command
oc set triggers bc/name --from-image=project/image:tag

** To remove an image change trigger from a build configuration
 oc set triggers bc/name --from-image=project/image:tag --remove

** Add a GitLab webhook to a build configuration:
oc set triggers bc/name --from-gitlab

** Remove
oc set triggers bc/name --from-gitlab --remove

** Create secret to acess de quay registry
oc create secret generic quay-registry \
--from-file .dockerconfigjson=${XDG_RUNTIME_DIR}/containers/auth.json \
--type kubernetes.io/dockerconfigjson

** Add the Quay.io registry secret to the builder service account.
oc secrets link builder quay-registry



** Update the php image stream to fetch the metadata for the new container image. The external registry uses the docker-distribution package and does not notify Red Hat OpenShift about image changes.
oc import-image php --from quay.io/${RHT_OCP4_QUAY_USER}/php-73-ubi8 --confirm


------ Implementing Post-commit Build Hooks-----
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.6/html-single/builds/triggering-builds-build-hooks
A typical scenario to use a post-commit build hook is to execute some tests in your application. This way, before RHOCP pushes the image to the registry and starts a new deployment, tests can check if the application is working correctly. If the test fails, the build fails and does not continue with a deployment.
There are a few other common use cases where it can be useful to leverage a post-commit build hook. For example:
To integrate a build with an external application through an HTTP API.
To validate a non-functional requirement such as application performance, security, usability, or compatibility.
To send an email to the developer's team informing them of the new build

** A command is executed using the exec system call. 
Create a command post-commit build hook using the --command option as shown 
in the following command:
oc set build-hook bc/name --post-commit \
--command -- bundle exec rake test --verbose

** Runs a build hook with the /bin/sh -ic command. This is more convenient 
since it has all the features provided by a shell, such as argument expansion, 
redirection, and so on. It only works if the base image has the sh shell. 
Create a shell script post-commit build hook using the --script as shown in 
the following command:
oc set build-hook bc/name --post-commit \
--script="curl http://api.com/user/${USER}"



-- Deploy a new application using version ~
oc new-app --name hook --context-dir post-commit \
php:7.3~http://github.com/${RHT_OCP4_GITHUB_USER}/DO288-apps

------------------------------------------------------
Describing the Source-to-Image Architecture
------------------------------------------------------
* assemble	 
The assemble script builds the application from source and places it into appropriate directories inside the image.

* run	       
The run script executes your application. It is recommended to use the exec command when running any container processes in your run script. This ensures 
signal propagation and graceful shutdown of any process launched by the run script.

* save-artifacts	
The save-artifacts script gathers all dependencies required for the application and saves them to a tar file to speed up subsequent builds. For example, for Ruby, gems installed by Bundler, or for Java, .m2 contents. This means that the build does not have to redownload these contents, improving build time. These dependencies are gathered into a tar file and streamed to the standard output.

* usage
The usage script provides a description of how to properly use your image.

* test/run	
The test/run script allows you to create a simple process to verify if the image is working correctly.




--------- Customizing an Existing S2I Base Image ------
-- Use the podman pull command to pull the container image from a conatiner registry to the local system. 
* podman pull myregistry.example.com/rhscl/php-73-rhel7

-- Use the podman inspect command to get the value of the io.openshift.s2i.scripts-url label, in order to determine the default location of the S2I scripts in the image.
* podman inspect --format='{{ index .Config.Labels "io.openshift.s2i.scripts-url"}}' rhscl/php-73-rhel7

-- Create a wrapper for the assemble script in the .s2i/bin folder
#!/bin/bash
echo "Making pre-invocation changes..."
/usr/libexec/s2i/assemble
rc=$?
if [ $rc -eq 0 ]; then
    echo "Making post-invocation changes..."
else
    echo "Error: assemble failed!"
fi
exit $rc

-- Similarly, create a wrapper for the run script in the .s2i/bin folder:
-- wrapping the run script, you must use exec to invoke it.
#!/bin/bash
echo "Before calling run..."
exec /usr/libexec/s2i/run

---- Incremental Builds in S2I ----
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.10/html-single/images/index#creating-images
https://cloud.redhat.com/blog/override-s2i-builder-scripts

The S2I build process provides a mechanism to reduce build time by invoking the save-artifacts script after invoking the assemble script
Dependent artifacts (libraries and components required for the application) are saved for future builds
The save-artifacts script output should only include the tar stream output, and nothing else
You should redirect output of other commands in the script to /dev/null.

The save-artifacts script that caches Maven JAR files can be defined as follows:
#!/bin/sh -e
# Stream the .m2 folder tar archive to stdout
if [ -d ${HOME}/.m2 ]; then
    pushd ${HOME} > /dev/null
    tar cf - .m2
    popd > /dev/null
fi

The corresponding code to restore the artifacts before building is in the assemble script:
# Restore the .m2 folder
...output omitted...
if [ -d /tmp/artifacts/.m2 ]; then
  echo "---> Restoring maven artifacts..."
  mv /tmp/artifacts/.m2 ${HOME}/
fi



- Run the image
podman run --name test -it quay.io/centos7/httpd-24-centos7 bash
- Inspect the S2I scripts
bash-4.2$ cat /usr/libexec/s2i/assemble
bash-4.2$ cat /usr/libexec/s2i/run
bash-4.2$ cat /usr/libexec/s2i/usage


--- Deploy the application to a RHOCP cluster. Verify that the custom S2I scripts are executed.

marciodevelopment
marciodevelopment
marciodevelopement

oc new-app \
--name bonjour \
httpd:latest~https://github.com/marciodevelopment/DO288-apps#manage-builds \
--context-dir s2i-scripts



------------ Creating an S2I Base Image S05 ----------
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.10/html-single/images/index
https://cloud.redhat.com/blog/create-s2i-builder-image
https://github.com/openshift/source-to-image
https://github.com/openshift/source-to-image/blob/master/docs/cli.md

S2I build process  -> source code + s2i builder image -> application container
--- Installing the S2I Tool
To teste the image







************************
CRC
************************

-- stop openshift
crc stop

-- run the open shift
crc start

-- console
crc console

Log in as administrator:
  Username: kubeadmin
  Password: sIbn8-eAoMD-UYeSS-37r8J

Log in as user:
  Username: developer
  Password: developer


  $ eval $(crc oc-env)
  $ oc login -u developer https://api.crc.testing:6443
  
  





************************
LInux
************************

- 
https://console-openshift-console.apps-crc.testing

Dúvidas:
Teremos um registro de imagem externo ao openshift ?
